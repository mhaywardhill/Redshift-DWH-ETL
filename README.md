# S3 to Redshift ETL pipeline

A music streaming startup, Sparkify, has grown its user base and song database and wants to move its processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app and a directory with JSON metadata on the songs in their app.

In this project, I build an ETL pipeline that extracts data from S3, stages it in Redshift, and then transforms the data into a set of dimensional tables ready for their analytics team to continue finding insights into the songs users are listening to.

## Schema for Song Play Analysis

A star schema optimized for queries on song play analysis. The schema includes the following tables.


### Staging tables

song_staging - dataset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. 
* num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year

log_staging - dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs dataset. These simulate app activity logs from an imaginary music streaming app based on configuration settings.
* artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId


### Fact Table

songplays - records in event data associated with song plays, i.e. records with page NextSong
* songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension Tables
users - users in the app
* user_id, first_name, last_name, gender, level

songs - songs in music database
* song_id, title, artist_id, year, duration

artists - artists in music database
* artist_id, name, location, lattitude, longitude

time - timestamps of records in songplays broken down into specific units
* start_time, hour, day, week, month, year, weekday

## Setup the Environment

### 1. Set environments variables:

```
HISTCONTROL=ignoreboth
 export DWH_ENDPOINT=<Redshift endpoint>
 export DWH_DB_PASSWORD=<Redshift password>
```

### 2. Setup the Python virtual environment:

```
conda create -n sparkify python=3.9
conda activate sparkify
make install
```  

### 3. Create tables in Redshift

```
python ./create_tables.py
```

### 4. Copy data from S3 to Redshift

```
python ./etl.py
```



## Cleanup Resources

```
conda deactivate
conda env remove -n sparkify
```

